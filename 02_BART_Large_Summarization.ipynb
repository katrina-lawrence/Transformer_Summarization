{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Summarization\n",
    "Will use the transformers library from Hugging Face to summarize text. Hugging Face is an open-source platform and provides pre-trained models. Transformers are a type of autoregressive sequence models, which uses its own previous outputs as inputs to make predictions and generate text. What makes transformers so powerful is that they consider the whole sequence of words that came before the one they are predicting, and they are constantly updating the bank of previous words after each subsequent prediction. \n",
    "\n",
    " I will use the pre-trained BART model. The architecture of this model is encoder-decoder therefore it is well suited for text generation and sequence to sequence tasks like summarization, translation and paraphrasing. \n",
    "\n",
    "\n",
    "Will extend this project by building a transformer model from scratch and comparing the summerization results with the pre-trained transformer model from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization for Large Text\n",
    "BART has a max token input length of 1024. However, documents will often exceed this length, so further summarization techniques will need to be explored. To achieve large document summarization, the text will first be broken up into chunks of tokens that do not exceed 1024. These text chunks will then be individually summarized and appended to an aggregate summary. Once the summaries of the individual chunks have been generated, they will then be summarized to achieve a more precise final output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installations\n",
    "#!pip install transformers\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#loading the BART tokenizer and model \n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking up the larger document into chunks; the chunk size will be 1024 as that is the maz input of BART\n",
    "\n",
    "def chunk_text(text, chunk_size = 1024):\n",
    "\n",
    "    \"\"\"\n",
    "    Splits a long document into chunks that fit within the model's max token limit\n",
    "    \n",
    "    :param text: the original long text to be chunked\n",
    "    \n",
    "    :param chunk_size: max number of tokens for each chunk\n",
    "    \n",
    "    :return: list of text chunks that fit the token size\n",
    "    \"\"\"\n",
    "\n",
    "    #tokenizing the text into token IDs that can be processed by the model \n",
    "    inputs = tokenizer.encode(text, return_tensors='pt', truncation=False)\n",
    "    total_tokens = inputs.size(1)\n",
    "\n",
    "\n",
    "    #splitting the tokenized input into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, total_tokens, chunk_size):\n",
    "        chunk = inputs[:, i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarizing each chunk in the chunk's list using the BART model\n",
    "\n",
    "def summarize_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Summarizes a chunk of text using the BART model\n",
    "    \n",
    "    :param chunk: A chunk of tokenized input\n",
    "    :return: the summary of the given chunk\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #summary_ids holds the token indices (IDs) of the summary generated by the model\n",
    "    #it is a tensor containing the IDs of the generated tokens\n",
    "    #these token IDs are the predicted tokens that for the summary text\n",
    "    ''' \n",
    "    max_length specifies the max number of tokens\n",
    "    min_length specifies the min number of tokens\n",
    "    length_penalty controls how much the model favours longer outputs (>1 encourages shorter outputs)\n",
    "    num_beams the number of candidate sequences the model keeps at each step (higher = higher quality)\n",
    "    early_stopping stops the generation process when all candidate sequences (beams) finish before reachine max_length\n",
    "    '''\n",
    "    summary_ids = model.generate(chunk, max_length = 150, min_length = 30,\n",
    "                                 length_penalty = 2.0, num_beams = 4, early_stopping = True)\n",
    "    \n",
    "\n",
    "    #need to decode the token IDs of the summary to convert them back into human-readable text\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    #returning the human-readable summary\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining the above functions\n",
    "#summarize a long document by splitting it up into chunks and summarizing each chunk\n",
    "\n",
    "def summarize_document(text, chunk_size = 1024):\n",
    "    \n",
    "    \"\"\"\n",
    "    Summarizes a long document by splitting it into chunks, summarizing each, and combining the summaries.\n",
    "    \n",
    "    :param text: The entire long document to summarize.\n",
    "    :param chunk_size: Maximum number of tokens per chunk.\n",
    "    :return: Combined summary of the entire document.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #use the chunk_text function to split up the text\n",
    "    text_chunks = chunk_text(text, chunk_size)\n",
    "\n",
    "    #summarize each chunk using the summarize_chunk definition\n",
    "    summaries = []\n",
    "    for chunk in text_chunks:\n",
    "        summary = summarize_chunk(chunk)\n",
    "        summaries.append(summary)\n",
    "\n",
    "\n",
    "    #combining the summaries\n",
    "    full_summary = \" \".join(summaries)\n",
    "\n",
    "    return full_summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking all of the summaries of the chunks and generating a final concise summary\n",
    "\n",
    "def concise_summary(summaries, max_length = 250):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes the chunk summaries and generates a final concise summary.\n",
    "    \n",
    "    :param summaries: The combined summaries from all chunks.\n",
    "    :param max_length: The maximum length of the final summary.\n",
    "    :return: The final summarized version of the summaries.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #Tokenize the summaries into a single string (encoding the summaries)\n",
    "    inputs = tokenizer.encode(\"summarize: \" + summaries, return_tensors = 'pt', \n",
    "                             max_length = 1024, truncation = True)\n",
    "    \n",
    "    #Summarize the combined summaries using the BART model\n",
    "    summary_ids = model.generate(inputs, max_length = max_length, min_length = 50, \n",
    "                                 length_penalty = 2.0, num_beams = 4, early_stopping = True)\n",
    "    \n",
    "    #Decode the final summary so that it is human-readable\n",
    "    final_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    #return the final summary\n",
    "    return final_summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
