{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model for Summarization\n",
    "The achieve a higher quality and more accurate summary, this approach will focus on combining multiple model outputs during inference instead of after summarization. This can be done in two main ways: taking the average of the token probabilities of the different models or using a majority voting mechanism to deterine the final summary.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Averaging Token Porbabilities\n",
    "This method will focus on averaging the token probabilities.\n",
    "\n",
    "Step 1) Generate token probabilities from the different models.\n",
    "\n",
    "Step 2) Average the probabilities for each token at every step.\n",
    "\n",
    "Step 3) Pick the tocken with the highest average probability at each step to form the summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installations\n",
    "#!pip install transformers sentence-transformers\n",
    "#!pip install tf-keras\n",
    "#!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\lawre\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\lawre\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lawre\\.cache\\huggingface\\hub\\models--google--pegasus-cnn_dailymail. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#loading the BART and Pegasus models and tokenizers\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-cnn_dailymail')\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lawre\\.cache\\huggingface\\hub\\models--t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#loading the t5 model and tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging the Logits\n",
    "Each model generates token logits, these are the probabilities of the next token. We want to average the logits (probabilities) of all models at each decoding step. The final token selection, as in the next token chosen for the summary, is determined by taking the token with the highest average probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are encountering an error with this method since the size of the logits from BART, Pegasus, and T5 models are not the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to return the logits of the models\n",
    "\n",
    "def get_model_logits(text, model, tokenizer, max_length = 1024):\n",
    "    \n",
    "    #converting the plain text into tokens which can be interpreted by the model (encoding the text)\n",
    "    inputs = tokenizer.encode(text, return_tensors = 'pt', max_length = max_length, truncation = True)\n",
    "    \n",
    "    # Prepare decoder input IDs for the models (typically the start token for summarization)\n",
    "    decoder_start_token = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    decoder_input_ids = torch.tensor([[decoder_start_token]], dtype=torch.long)\n",
    "\n",
    "    #passing the model the tokenized text\n",
    "    outputs = model(input_ids=inputs, decoder_input_ids=decoder_input_ids, return_dict=True)\n",
    "\n",
    "    #returning the token logits and the tokens\n",
    "    return outputs.logits, inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to generate a summary based on the averaging the token probabilities\n",
    "\n",
    "def ensemble_generate(text, max_length = 150, min_length = 30, num_beams = 4, early_stopping = True):\n",
    "\n",
    "    #retrieving the logits from BART, Pegasus, and T5 using our above defined function\n",
    "    bart_logits, inputs_bart = get_model_logits(text, bart_model, bart_tokenizer)\n",
    "    pegasus_logits, inputs_pegasus = get_model_logits(text, pegasus_model, pegasus_tokenizer)\n",
    "    t5_logits, inputs_t5 = get_model_logits(text, t5_model, t5_tokenizer)    \n",
    "\n",
    "\n",
    "    #taking the average of the logits\n",
    "    combined_logits = (bart_logits + pegasus_logits + t5_logits) / 3\n",
    "\n",
    "    #generating tokens from the averaged logits\n",
    "    generated_tokens = torch.argmax(combined_logits, dim=-1)\n",
    "\n",
    "    #decoding the generated tokens to a readable summary \n",
    "    final_summary = bart_tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    #returning the final summary\n",
    "    return final_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (50264) must match the size of tensor b (96103) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m long_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mArtificial Intelligence (AI) has been a major breakthrough in technology. It is being utilized in various domains such as healthcare, autonomous driving, and natural language processing...\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m... [continued long text]\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#genetating a summary using this ensemble model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m final_summary \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlong_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Summary After Model Ensembling:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, final_summary)\n",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m, in \u001b[0;36mensemble_generate\u001b[1;34m(text, max_length, min_length, num_beams, early_stopping)\u001b[0m\n\u001b[0;32m      8\u001b[0m t5_logits, inputs_t5 \u001b[38;5;241m=\u001b[39m get_model_logits(text, t5_model, t5_tokenizer)    \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#taking the average of the logits\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m combined_logits \u001b[38;5;241m=\u001b[39m (\u001b[43mbart_logits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpegasus_logits\u001b[49m \u001b[38;5;241m+\u001b[39m t5_logits) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#generating tokens from the averaged logits\u001b[39;00m\n\u001b[0;32m     15\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(combined_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (50264) must match the size of tensor b (96103) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "#running an example with a long text document\n",
    "long_text = \"\"\"\n",
    "Artificial Intelligence (AI) has been a major breakthrough in technology. It is being utilized in various domains such as healthcare, autonomous driving, and natural language processing...\n",
    "... [continued long text]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#genetating a summary using this ensemble model\n",
    "final_summary = ensemble_generate(long_text)\n",
    "\n",
    "print(\"Final Summary After Model Ensembling:\\n\", final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to pad all logits so they are the size of the largest vocabulary (so their dimensions match). Run into an issue where NoneTypes are generated and cannot be properly prosessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 61\u001b[0m\n\u001b[0;32m     55\u001b[0m long_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124mArtificial Intelligence (AI) has been a major breakthrough in technology. It is being utilized in various domains such as healthcare, autonomous driving, and natural language processing...\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124m... [continued long text]\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Generate summary using ensemble method\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m final_summary \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlong_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Summary After Model Ensembling:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, final_summary)\n",
      "Cell \u001b[1;32mIn[28], line 50\u001b[0m, in \u001b[0;36mensemble_generate\u001b[1;34m(text, max_length, min_length, num_beams, early_stopping)\u001b[0m\n\u001b[0;32m     47\u001b[0m valid_tokens \u001b[38;5;241m=\u001b[39m generated_tokens[valid_token_mask]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Decode the valid tokens to readable summary\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m final_summary \u001b[38;5;241m=\u001b[39m \u001b[43mbart_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_summary\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:4016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   4013\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   4014\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 4016\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4020\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4021\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils.py:1104\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m         current_sub_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_sub_text:\n\u001b[1;32m-> 1104\u001b[0m     sub_texts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_sub_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spaces_between_special_tokens:\n\u001b[0;32m   1107\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sub_texts)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bart\\tokenization_bart.py:278\u001b[0m, in \u001b[0;36mBartTokenizer.convert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m    277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_decoder[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m text])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "def get_model_logits(text, model, tokenizer, max_length=1024):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    \n",
    "    # Prepare decoder input IDs (start token)\n",
    "    decoder_start_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    decoder_input_ids = torch.tensor([[decoder_start_token_id]], dtype=torch.long)\n",
    "\n",
    "    # Call model with encoder input and decoder start token\n",
    "    outputs = model(input_ids=inputs, decoder_input_ids=decoder_input_ids, return_dict=True)\n",
    "    \n",
    "    return outputs.logits, inputs\n",
    "\n",
    "# Function to pad logits to the maximum size\n",
    "def pad_logits(logits, target_size):\n",
    "    current_size = logits.size(-1)\n",
    "    if current_size < target_size:\n",
    "        # Pad the logits with zeros to match the target size\n",
    "        pad_size = target_size - current_size\n",
    "        logits = torch.nn.functional.pad(logits, (0, pad_size), value=0)\n",
    "    return logits\n",
    "\n",
    "# Ensemble function to average logits from different models\n",
    "def ensemble_generate(text, max_length=150, min_length=30, num_beams=4, early_stopping=True):\n",
    "    # Get logits from BART, Pegasus, and T5\n",
    "    bart_logits, inputs_bart = get_model_logits(text, bart_model, bart_tokenizer)\n",
    "    pegasus_logits, inputs_pegasus = get_model_logits(text, pegasus_model, pegasus_tokenizer)\n",
    "    t5_logits, inputs_t5 = get_model_logits(text, t5_model, t5_tokenizer)\n",
    "\n",
    "    # Find the largest vocabulary size among the models\n",
    "    max_vocab_size = max(bart_logits.size(-1), pegasus_logits.size(-1), t5_logits.size(-1))\n",
    "\n",
    "    # Pad the logits so they all match the largest size\n",
    "    bart_logits_padded = pad_logits(bart_logits, max_vocab_size)\n",
    "    pegasus_logits_padded = pad_logits(pegasus_logits, max_vocab_size)\n",
    "    t5_logits_padded = pad_logits(t5_logits, max_vocab_size)\n",
    "\n",
    "    # Average the padded logits\n",
    "    combined_logits = (bart_logits_padded + pegasus_logits_padded + t5_logits_padded) / 3\n",
    "\n",
    "    # Generate tokens from averaged logits\n",
    "    generated_tokens = torch.argmax(combined_logits, dim=-1)\n",
    "\n",
    "    # Apply a mask to remove padding or invalid tokens\n",
    "    valid_token_mask = generated_tokens != bart_tokenizer.pad_token_id\n",
    "\n",
    "    # Keep only valid tokens for decoding\n",
    "    valid_tokens = generated_tokens[valid_token_mask]\n",
    "\n",
    "    # Decode the valid tokens to readable summary\n",
    "    final_summary = bart_tokenizer.decode(valid_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# Example long text document (scientific or article)\n",
    "long_text = \"\"\"\n",
    "Artificial Intelligence (AI) has been a major breakthrough in technology. It is being utilized in various domains such as healthcare, autonomous driving, and natural language processing...\n",
    "... [continued long text]\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary using ensemble method\n",
    "final_summary = ensemble_generate(long_text)\n",
    "\n",
    "print(\"Final Summary After Model Ensembling:\\n\", final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using a Ranking or Voting Mechcanism\n",
    "Selecting the best sentence from the original summaries instead of averaging the logits. This method relies on embedding the sentences and calculating the cosine similarity between the sentence embeddings of the original text and the sentence embeddings of the generated summaries. Essentially, each model will independently generate a summary which will subsequently be ranked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary After Model Ensembling:\n",
      " Artificial Intelligence (AI) has been a major breakthrough in technology. It is being utilized in various domains such as healthcare, autonomous driving, and natural language processing.\n"
     ]
    }
   ],
   "source": [
    "# Sentence embedding model for ranking\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Function to generate summary from a model\n",
    "def generate_summary(text, model, tokenizer, max_length=150, min_length=30, num_beams=4, early_stopping=True):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, num_beams=num_beams, early_stopping=early_stopping)\n",
    "    \n",
    "    # Decode and return the summary\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Ensemble function to rank summaries\n",
    "def ensemble_generate(text):\n",
    "    # Generate summaries from all models\n",
    "    bart_summary = generate_summary(text, bart_model, bart_tokenizer)\n",
    "    pegasus_summary = generate_summary(text, pegasus_model, pegasus_tokenizer)\n",
    "    t5_summary = generate_summary(text, t5_model, t5_tokenizer)\n",
    "    \n",
    "    # Get sentence embeddings for the original text and each summary\n",
    "    original_embedding = embedder.encode(text, convert_to_tensor=True)\n",
    "    summaries = [bart_summary, pegasus_summary, t5_summary]\n",
    "    summary_embeddings = embedder.encode(summaries, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarity between the original text and each summary\n",
    "    similarities = util.pytorch_cos_sim(original_embedding, summary_embeddings)\n",
    "    \n",
    "    # Select the summary with the highest similarity score\n",
    "    best_summary_idx = torch.argmax(similarities).item()\n",
    "    \n",
    "    return summaries[best_summary_idx]\n",
    "\n",
    "# Example long text document (scientific or article)\n",
    "long_text = \"\"\"\n",
    "Artificial Intelligence (AI) has been a major breakthrough in technology. It is being utilized in various domains such as healthcare, autonomous driving, and natural language processing...\n",
    "... [continued long text]\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary using ensemble method\n",
    "final_summary = ensemble_generate(long_text)\n",
    "\n",
    "print(\"Final Summary After Model Ensembling:\\n\", final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
